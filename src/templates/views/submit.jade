.row.main
	
	.column-9.suffix-2(style='margin-left: 30px; margin-bottom: 50px')

		h2(style='margin-top:55px')
			| Submit an algorithm

		hr

		p
			| Our submission system is integrated with GitHub. To submit an algorithm, follow these steps:

		ul

			li
				span.list-number
					| 1
				span
					| Sign up for an account on 
					a(href='https://github.com/join')
						| GitHub 
			li
				span.list-number
					| 2
				span
					| Fork the 
					a(href='https://github.com/codeneuro/neurofinder')
						| NeuroFinder 
					| repository (see 
					a(href='https://help.github.com/articles/fork-a-repo/')
						| here 
					| for an explanation of forking)
			li
				span.list-number
					| 3
				span
					| Create a new branch of the repository (name the branch with your username)
			li
				span.list-number
					| 4
				span
					| Add the folder 
					span.inline-code
						| neurofinder/submissions/user-name-algorithm-name
					| with the structure described below
			li
				span.list-number
					| 5
				span
					| Submit your branch as a pull request and wait for your algorithm to be validated and run!

		p
			| Jobs will be validated and executed frequently, but please be patient! You will be notified of updates or any errors through comments on your pull request. 

		hr

		h3
			| Submission structure

		p
			| We have standardized a format for input data and algorithm outputs, partly based on classes from the 
			a(href='http://thunder-project.org/thunder/docs/install_ec2.html')
				| Thunder  
			| library. Your submission needs to provide a Python function called 
			span.inline-code 
				| run
			| that takes an 
			span.inline-code 
				| Images
			| object as input (an array of images) and returns a
			span.inline-code 
				| SourceModel
			| as output (a collection of coordinates). You should organize this function in a module as follows

			pre
				code.
					neurofinder/submissions/user-name-algorithm-name/info.json
					neurofinder/submissions/user-name-algorithm-name/run/run.py
					neurofinder/submissions/user-name-algorithm-name/run/__init__.py

		p
			| The file 
			span.inline-code
				| info.json
			| should contain the following simple metadata about your submission:

		pre
				code.
					{
						"algorithm": "name of your algorithm",
						"description": "description of your algorithm"
					}

		p
			| The file
			span.inline-code
				| run.py
			| should contain a function of the following form
			
		pre
			code.
				def run(data, info):
						"""
						Run a source extraction algorithm on an image sequence.

						Parameters
						----------
						data : thunder.rdds.images.Images
							The data the algorithm will be run on

						info : dict
							Metadata associated with the dataset

						Returns
						-------
						model : thunder.extraction.source.SourceModel
								Sources found by the algorithm as a SourceModel object
						"""

						# your code here

						return model
		p
			| See the 
			a(href='https://github.com/CodeNeuro/neurofinder/tree/master/submissions/example-user')
				| test submission 
			| for a complete example.

		p
			| Your function can do whatever operations are required by your algorithm, so long as it runs within this environment. We encourage algorithms that use Thunder's existing Source Extraction API. For example, many algorithms operate on spatio-temporal blocks of a data set, and you can implement a new one just by specifying the function to apply to each block.

		hr

		h3
			| Environment

		p
			| All jobs will be run on an Amazon EC2 cluster, accessing data from S3, in a standardized environment with 
			a(href='http://spark-project.org/')
				| Spark 
			| , 
			a(href='http://thunder-project.org/thunder/docs-dev/')
				| Thunder
			|  , and the 
			a(href='https://store.continuum.io/cshop/anaconda/')
				| Anaconda
			|  distribution of Python. The following are included libraries and specs:

		ul

		li.list-name
			| Python v2.7.6

		li.list-name
			| Spark v1.3.1

		li.list-name
			| Numpy v1.9.2

		li.list-name
			| Scipy v0.15.1

		li.list-name
			| Scikit Learn v0.16.1

		li.list-name
			| Scikit Image v0.10.1

		li.list-name
			| Matplotlib v1.4.3

		br

		p
			| We offer a 
			a(href='http://notebooks.codeneuro.org')
				| notebook service 
			| that uses 
			a(href='https://www.docker.com/')
				| Docker 
			| containers to deploy an interactive version of this environment running in Jupyter notebooks. This is useful for testing and developing algorithms, but is currently limited to only one node so is most useful for testing on subsets of the data. 

		p
			| For more scalable testing, you can start your own EC2 cluster with the same environment with Spark and Thunder's EC2 deployment scripts, see the 
			a(href='http://thunder-project.org/thunder/docs-dev/install_ec2.html')
				| instructions 
			| . If an algorithm requires additional dependencies, we should be able to add them, especially pure Python packages. Just reach out to us on 
			a(href='http://twitter.com/codeneuro')
				| twitter 
			| or through the 
			a(href='https://gitter.im/CodeNeuro/neurofinder')
				| chatroom
			| .

		hr

		h3
			| Training

		p
			| Training data is available through the 
			a(href='http://datasets.codeneuro.org')
				| CodeNeuro data repository  
			| under the codeneuro/challenges category. The best way to get started is to use our 
			a(href='http://notebooks.codeneuro.org')
				| notebook 
			|	service to start an interactive environment for testing data loading and prototyping analyses. For heavier testing, you'll want to start a small cluster on Amazon EC2 (we recommend ~5 nodes) and run the same code.

		p
			| From an instance or notebook running on EC2 with Thunder installed, loading images and sources is easy:

		pre
			code.
				path = "s3n://neuro.datasets/challenges/neurofinder/01.00/"
				imgs = tsc.loadImages(path + "images")
				sources = tsc.loadSources(path + "sources/sources.json")

		p
			| Given the size of the data sets, we do not support direct downloads of all training data, but we may add direct downloads of small subsets for local testing.

		hr

		h3
			| Testing

		p
			| Algorithms will be run on a subset of held-out test data, similar in kind to the training examples. The following metrics will be computed for each dataset and algorithm (higher is better for all):

		ul

		p
			span.inline-code
				| recall
			| is the fraction of true sources for which a source was correctly identified (within a distance threshold)

		p
			span.inline-code
				| precision
			| is the fraction of estimated sources that correctly identified a true source (within a distance threshold)

		p
			span.inline-code
				| score
			| combines recall and precision into a single number (via a harmonic mean)

		p
			span.inline-code
				| overlap
			| is the fraction of correct pixel overlap for those sources that were correctly identified

		p
			span.inline-code
				| exactness
			| is the precision associated with pixel overlap

		br

		p
			| You can read more about these metrics by browsing Thunder's 
			a(href='http://thunder-project.org/thunder/docs-dev/')
				| documentation 
			| , in particular the 
			span.inline-code
				| Source
			| and 
			span.inline-code
				| SourceModel
			| classes. 